{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model creation playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import itertools\n",
    "from pickle import dump\n",
    "import csv\n",
    "import sys\n",
    "import string\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14786 dialogues by Jerry in the all Seinfeld episodes!\n",
      "\n",
      "Few dialogues by Jerry:\n",
      "['You do of course try on, when you buy?', 'Oh, you dont recall?', 'Well, senator, Id just like to know, what you knew and when you knew it.']\n"
     ]
    }
   ],
   "source": [
    "# We will just use dialogue by Jerry\n",
    "dialogues_by_jerry = []\n",
    "with open('input_files/complete_ seinfeld_scripts.csv') as input_file:\n",
    "    input_data = csv.DictReader(input_file)\n",
    "    for row in input_data:\n",
    "        if(row['Character'] == 'JERRY'):\n",
    "            dialogues_by_jerry.append(row['Dialogue'])\n",
    "print(\"There are {} dialogues by Jerry in the all Seinfeld episodes!\\n\".format(len(dialogues_by_jerry)))\n",
    "print(\"Few dialogues by Jerry:\")\n",
    "print(dialogues_by_jerry[2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vectorizing prefix and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a text corpus\n",
    "text_corpus = \" \".join(dialogues_by_jerry)\n",
    "#print(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 unique characters in the corpus:\n",
      "['\\t', '\\n', '\\x0b', '\\x0c', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# List of unique characters in the corpus\n",
    "unique_chars = sorted(string.printable)\n",
    "\n",
    "unique_chars_length = len(unique_chars)\n",
    "print(\"There are {} unique characters in the corpus:\".format(unique_chars_length))\n",
    "print(unique_chars)\n",
    "\n",
    "# Assign a unique index to each unique character\n",
    "unique_chars_mapping = dict((char, i) for i, char in enumerate(unique_chars))\n",
    "\n",
    "# Save the mapping\n",
    "dump(unique_chars_mapping, open('saved_models/unique_chars_mapping.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 784299 prefixes and 784299 targets, respectively.\n"
     ]
    }
   ],
   "source": [
    "# Get sequence of max_len characters\n",
    "max_len = 40\n",
    "\n",
    "# Sample new sequence every sequence_step characters\n",
    "sequence_step = 1\n",
    "\n",
    "# Lists to hold the prefixes and targets\n",
    "prefix_sequences = []\n",
    "target_character = []\n",
    "for i in range (0, len(text_corpus) - max_len, sequence_step):\n",
    "    prefix_sequences.append(text_corpus[i: i + max_len])\n",
    "    target_character.append(text_corpus[i + max_len])\n",
    "print(\"There are {} prefixes and {} targets, respectively.\".format(len(prefix_sequences), \n",
    "                                                                   len(target_character)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prefix_sequences_encoded = []\n",
    "for sequence in prefix_sequences:\n",
    "    prefix_sequences_encoded.append([unique_chars_mapping[char] for char in sequence])\n",
    "\n",
    "target_character_encoded = []\n",
    "for char in target_character:\n",
    "    target_character_encoded.append(unique_chars_mapping[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = [to_categorical(x, num_classes=unique_chars_length) for x in prefix_sequences_encoded]\n",
    "X = np.array(X)\n",
    "\n",
    "y = to_categorical(target_character_encoded, num_classes=unique_chars_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split data into training, validation, test sets (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 627439 samples.\n",
      "The validation set has 78430 samples.\n",
      "The test set has 78430 samples.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=99)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=99)\n",
    "\n",
    "print(\"The training set has %d samples.\" % len(X_train))\n",
    "print(\"The validation set has %d samples.\" % len(X_val))\n",
    "print(\"The test set has %d samples.\" % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 130,148.0\n",
      "Trainable params: 130,148\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128,\n",
    "                      dropout=0.1,\n",
    "                      recurrent_dropout=0.1,\n",
    "                      input_shape=(max_len, unique_chars_length)))\n",
    "model.add(layers.Dense(unique_chars_length, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compile the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.01), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the model (before training): 0.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test accuracy before training\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy of the model (before training): %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# Stop training when a monitored quantity has stopped improving after 20 epochs\n",
    "early_stop = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.3, patience=3, cooldown=3, verbose=1)\n",
    "\n",
    "# Save the best model after every epoch\n",
    "check_point = ModelCheckpoint(filepath='saved_models/model_weights.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0,\n",
    "                             validation_data=(X_val, y_val), \n",
    "                             callbacks=[check_point, early_stop, reduce_lr])\n",
    "\n",
    "\n",
    "# Summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.savefig('plots/model_accuracy.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.savefig('plots/model_loss.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model.save('saved_models/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the test accuracy after the training\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy of the model (after training): %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test the real time auto-complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_test = load_model('saved_models/model.hdf5')\n",
    "model_test.load_weights('saved_models/model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "break_at_char = [\"?\", \".\", \"!\"]\n",
    "\n",
    "test_text = \"What is the deal\"\n",
    "temperature = 0.2\n",
    "\n",
    "print('Test text:', test_text)\n",
    "\n",
    "outputs = [test_text]\n",
    "for _ in range(50):\n",
    "    test_text_encoded = [unique_chars_mapping[char] for char in test_text]\n",
    "    \n",
    "    # Truncate sequences to a fixed length\n",
    "    test_text_encoded = pad_sequences([test_text_encoded], maxlen=max_len, truncating='pre')\n",
    "    \n",
    "    # One hot encode\n",
    "    test_text_encoded = to_categorical(test_text_encoded, num_classes=unique_chars_length)\n",
    "    test_text_encoded = test_text_encoded.reshape(1, test_text_encoded.shape[0], test_text_encoded.shape[1])\n",
    "    \n",
    "    preds = model_test.predict(test_text_encoded, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = unique_chars[next_index]\n",
    "    outputs.append(next_char)\n",
    "    \n",
    "    test_text += next_char\n",
    "    if next_char in break_at_char:\n",
    "        break\n",
    "            \n",
    "print(\"\".join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envAutoComplete",
   "language": "python",
   "name": "envautocomplete"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
