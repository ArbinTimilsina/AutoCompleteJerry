{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24073 sentences by Jerry in all Seinfeld episodes!\n",
      "\n",
      "Few words by Jerry:\n",
      "[['do', 'you', 'know', 'what', 'this', 'is', 'all', 'about', '?'], ['do', 'you', 'know', 'why', 'were', 'here', '?'], ['to', 'be', 'out', 'this', 'is', 'out', 'and', 'out', 'is', 'one', 'of', 'the', 'single', 'most', 'enjoyable', 'experiences', 'of', 'life', '.'], ['people', 'did', 'you', 'ever', 'hear', 'people', 'talking', 'about', 'we', 'should', 'go', 'out', '?'], ['this', 'is', 'what', 'theyre', 'talking', 'about', 'this', 'whole', 'thing', 'were', 'all', 'out', 'now', 'no', 'one', 'is', 'home', '.'], ['not', 'one', 'person', 'here', 'is', 'home', 'were', 'all', 'out'], ['there', 'are', 'people', 'tryin', 'to', 'find', 'us', 'they', 'dont', 'know', 'where', 'we', 'are', '.'], ['on', 'an', 'imaginary', 'phone', 'did', 'you', 'ring', '?', 'i', 'cant', 'find', 'him', '.'], ['where', 'did', 'he', 'go', '?'], ['he', 'didnt', 'tell', 'me', 'where', 'he', 'was', 'going', '.'], ['he', 'must', 'have', 'gone', 'out', '.'], ['you', 'wan', 'na', 'go', 'out', 'you', 'get', 'ready', 'you', 'pick', 'out', 'the', 'clothes', 'right', '?'], ['you', 'take', 'the', 'shower', 'you', 'get', 'all', 'ready', 'get', 'the', 'cash', 'get', 'your', 'friends', 'the', 'car', 'the', 'spot', 'the', 'reservation', 'then', 'youre', 'standing', 'around', 'whatta', 'you', 'do', '?'], ['you', 'go', 'we', 'got', 'ta', 'be', 'getting', 'back', '.'], ['once', 'youre', 'out', 'you', 'wan', 'na', 'get', 'back'], ['you', 'wan', 'na', 'go', 'to', 'sleep', 'you', 'wan', 'na', 'get', 'up', 'you', 'wan', 'na', 'go', 'out', 'again', 'tomorrow', 'right', '?'], ['where', 'ever', 'you', 'are', 'in', 'life', 'its', 'my', 'feeling', 'youve', 'got', 'ta', 'go', '.'], ['pointing', 'at', 'georges', 'shirt', 'see', 'to', 'me', 'that', 'button', 'is', 'in', 'the', 'worst', 'possible', 'spot', '.'], ['the', 'second', 'button', 'literally', 'makes', 'or', 'breaks', 'the', 'shirt', 'look', 'at', 'it', '.'], ['its', 'too', 'high'], ['its', 'in', 'nomansland', '.'], ['you', 'look', 'like', 'you', 'live', 'with', 'your', 'mother', '.'], ['you', 'do', 'of', 'course', 'try', 'on', 'when', 'you', 'buy', '?'], ['oh', 'you', 'dont', 'recall', '?'], ['well', 'senator', 'id', 'just', 'like', 'to', 'know', 'what', 'you', 'knew', 'and', 'when', 'you', 'knew', 'it', '.'], ['can', 'you', 'relax', 'its', 'a', 'cup', 'of', 'coffee', '.'], ['claire', 'is', 'a', 'professional', 'waitress', '.'], ['well', 'theres', 'this', 'uh', 'woman', 'might', 'be', 'comin', 'in', '.'], ['i', 'told', 'you', 'about', 'laura', 'the', 'girl', 'i', 'met', 'in', 'michigan', '?'], ['i', 'thought', 'i', 'told', 'you', 'about', 'it', 'yes', 'she', 'teaches', 'political', 'science', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Don't filter . and ?\n",
    "punctuations_filter='!\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    return sentence.translate(str.maketrans('', '', punctuations_filter))\n",
    "\n",
    "def remove_dots(sentence):\n",
    "    return sentence.replace('...', ' ')\n",
    "\n",
    "def remove_nonascii(word):\n",
    "    return ''.join([char if ord(char) < 128 else '' for char in word])\n",
    "\n",
    "def make_lower(word):\n",
    "    return word.lower()\n",
    "\n",
    "def clean_word(word):\n",
    "    processed = remove_nonascii(word)\n",
    "    processed = make_lower(processed)\n",
    "    if not processed.isdigit():\n",
    "        return processed\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# We will just use dialogue by Jerry\n",
    "input_file_path = path.join(\"..\", \"input_files\", \"complete_seinfeld_scripts.csv\")\n",
    "\n",
    "sentences_by_jerry = []\n",
    "with open(input_file_path) as input_file:\n",
    "    input_data = csv.DictReader(input_file)\n",
    "    for row in input_data:\n",
    "        if(row['Character'] == 'JERRY'):\n",
    "            for sentence in sent_tokenize(row['Dialogue']):\n",
    "                sentence = remove_dots(sentence)\n",
    "                sentence = remove_punctuations(sentence)\n",
    "                sentences_by_jerry.append([clean_word(word) for word in word_tokenize(sentence)])\n",
    "print(\"There are {} sentences by Jerry in all Seinfeld episodes!\\n\".format(len(sentences_by_jerry)))\n",
    "print(\"Few words by Jerry:\")\n",
    "print(sentences_by_jerry[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is sdfffd   '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations_filter='!\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~0123456789'\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    return sentence.translate(str.maketrans('', '', punctuations_filter))\n",
    "\n",
    "remove_punctuations(\"My name is sdf;ffd 10, 11, 12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is: 9928.\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(filters=punctuations_filter)\n",
    "\n",
    "# And build the word index\n",
    "tokenizer.fit_on_texts(sentences_by_jerry)\n",
    "\n",
    "# This is how we can recover the word index that was computed\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Vocabulary size\n",
    "vocabulary_size = len(word_index) + 1\n",
    "print(\"Vocabulary size is: {}.\".format(vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['do', 'you', 'know'], ['you', 'know', 'what'], ['know', 'what', 'this']]\n",
      "['what', 'this', 'is', 'all', 'about', '?', 'why', 'were', 'here', '?', 'this', 'is', 'out', 'and', 'out', 'is', 'one', 'of', 'the', 'single', 'most', 'enjoyable', 'experiences', 'of', 'life', '.', 'ever', 'hear', 'people', 'talking', 'about', 'we', 'should', 'go', 'out', '?', 'theyre', 'talking', 'about', 'this', 'whole', 'thing', 'were', 'all', 'out', 'now', 'no', 'one', 'is', 'home', '.', 'here', 'is', 'home', 'were', 'all', 'out', 'tryin', 'to', 'find', 'us', 'they', 'dont', 'know', 'where', 'we', 'are', '.', 'phone', 'did', 'you', 'ring', '?', 'i', 'cant', 'find', 'him', '.', 'go', '?', 'me', 'where', 'he', 'was', 'going', '.', 'gone', 'out', '.', 'go', 'out', 'you', 'get', 'ready', 'you', 'pick', 'out', 'the', 'clothes', 'right']\n",
      "100853\n",
      "100853\n"
     ]
    }
   ],
   "source": [
    "sequence_max_len = 3\n",
    "\n",
    "prefix_word = []\n",
    "target_word = []\n",
    "for dialogue in sentences_by_jerry:\n",
    "    for i in range (len(dialogue) - sequence_max_len):\n",
    "        prefix_word.append(dialogue[i: i + sequence_max_len])\n",
    "        target_word.append(dialogue[i + sequence_max_len])\n",
    "\n",
    "print(prefix_word[0:3])\n",
    "print(target_word[0:100])\n",
    "\n",
    "print(len(prefix_word))\n",
    "print(len(target_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9], [15], [14], [40], [45], [2], [69], [78], [61], [2], [15], [14], [37], [11], [37], [14], [77], [12], [4], [1176], [445], [4804], [4805], [12], [192], [1], [179], [241], [127], [188], [45], [49], [138], [53], [37], [2], [113], [188], [45], [15], [161], [97], [78], [40], [37], [81], [17], [77], [14], [267], [1], [61], [14], [267], [78], [40], [37], [2111], [7], [256], [190], [51], [19], [16], [112], [49], [41], [1], [137], [66], [3], [1817], [2], [5], [73], [256], [63], [1], [53], [2], [20], [112], [29], [35], [71], [1], [543], [37], [1], [53], [37], [3], [38], [471], [3], [298], [37], [4], [431], [50]]\n",
      "[[23, 3, 16], [3, 16, 9], [16, 9, 15]]\n",
      "[9, 15, 14]\n",
      "100853\n",
      "100740\n"
     ]
    }
   ],
   "source": [
    "# This turns strings into lists of integer indices.\n",
    "prefix_sequences = tokenizer.texts_to_sequences(prefix_word)\n",
    "target = tokenizer.texts_to_sequences(target_word)\n",
    "print(target[:100])\n",
    "\n",
    "target_sequences = []\n",
    "for sequence in target:\n",
    "    for seq in sequence:\n",
    "        target_sequences.append(seq)\n",
    "\n",
    "print(prefix_sequences[0:3])\n",
    "print(target_sequences[0:3])\n",
    "print(len(prefix_sequences))\n",
    "print(len(target_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prefix_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-23e06401afe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prefix_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Normalize\n",
    "X = np.array(prefix_sequences)\n",
    "print(X.shape)\n",
    "y = np.array(target_sequences)\n",
    "print(y.shape)\n",
    "\n",
    "X = X / float(vocabulary_size)\n",
    "y = to_categorical(y, num_classes=vocabulary_size)\n",
    "print(y[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Options are 50, 100, 200, 300\n",
    "embedding_dim = 100\n",
    "\n",
    "glove_dir = 'glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(path.join(\"..\", glove_dir, 'glove.6B.{}d.txt'.format(embedding_dim)))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < vocabulary_size:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-305f7bd4bcf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The training set has %d samples.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=99)\n",
    "\n",
    "print(\"The training set has %d samples.\" % len(X_train))\n",
    "print(\"The validation set has %d samples.\" % len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 3, 100)       1002100     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 3, 100)       0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 3, 100)       80400       dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 3, 100)       80400       lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 3, 300)       0           dropout_20[0][0]                 \n",
      "                                                                 lstm_22[0][0]                    \n",
      "                                                                 lstm_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  (None, 100)          160400      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 100)          10100       lstm_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 100)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 10021)        1012121     dropout_21[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,345,521\n",
      "Trainable params: 1,343,421\n",
      "Non-trainable params: 1,002,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Input, Embedding, Dropout, LSTM, Dense\n",
    "\n",
    "regularizer = l2(1e-4)\n",
    "dropout = 0.2\n",
    "\n",
    "inputs = Input(shape=(sequence_max_len,))\n",
    "embedding = Embedding(vocabulary_size, embedding_dim, input_length=sequence_max_len)(inputs)\n",
    "embedding = Dropout(dropout)(embedding)\n",
    "\n",
    "LSTM1 = LSTM(embedding_dim,\n",
    "               return_sequences=True,\n",
    "               dropout=dropout, recurrent_dropout=dropout,\n",
    "               kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer)(embedding)\n",
    "LSTM2 = LSTM(embedding_dim,\n",
    "               return_sequences=True,\n",
    "               dropout=dropout, recurrent_dropout=dropout,\n",
    "               kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer)(LSTM1)\n",
    "\n",
    "concat = concatenate([embedding, LSTM1, LSTM2])\n",
    "\n",
    "LSTM3 = LSTM(embedding_dim,\n",
    "               return_sequences=False,\n",
    "               dropout=dropout, recurrent_dropout=dropout,\n",
    "               kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer)(concat)\n",
    "\n",
    "dense1 = Dense(embedding_dim, activation='relu', kernel_regularizer=regularizer, bias_regularizer=regularizer)(LSTM3)\n",
    "dense1 = Dropout(dropout)(dense1)\n",
    "\n",
    "outputs = Dense(vocabulary_size, activation='softmax')(dense1)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "model.layers[1].set_weights([embedding_matrix])\n",
    "model.layers[1].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=1E-5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Stop training when a monitored quantity has stopped improving after 20 epochs\n",
    "early_stop = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.3, patience=3, cooldown=3, verbose=1)\n",
    "\n",
    "# Save the best model after every epoch\n",
    "check_point = ModelCheckpoint(filepath='../saved_models/model_weights.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                             validation_data=(X_val, y_val), \n",
    "                             callbacks=[check_point, early_stop, reduce_lr])\n",
    "\n",
    "# Summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.savefig('../plots/model_accuracy.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.savefig('../plots/model_loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envAutoCompleteJerry",
   "language": "python",
   "name": "envautocompletejerry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
